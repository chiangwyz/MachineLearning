# 支持向量机(核方法)

在深度学习出现之前，使用了核方法的支持向量机非常受欢迎。通过在支持向量机中引入核方法(kernel methods)这个技巧，那些无法人力标注特征值的数据也能被处理。

## 概述

在向量机中引入核方法，使得模型可以学习复杂的决策边界的做法。LSVM通过最大化间隔，所以得到尽可能远离数据的“好的”决策边界，但是由于决策边界必定为直线，所以它很难对“每个标签的边界为曲线的数据”进行分类。

## 算法说明

核方法的一个常见解释是“将数据移动到另一个特征空间，然后进行线性回归”。

首先，思考一下如何将线性不可分数据成为线性可分数据，假设有一个比训练数据更高维的空间，训练数据中的每一个点都对应着这个高维空间中的一个点，在这个高维空间中，训练数据对应的点是可以线性分离的，实际的训练数据是来自于该高维空间的投影。一旦有了这样的空间，模型就可以在高维空间中使用支持向量机来学习决策边界。最后，将高维空间的决策边界投影到由原始特征形成的向量空间上，得到决策边界。

虽然构建线性分离的高维空间非常困难，但通过一个叫做核函数的函数，核方法就可以使用在高维空间中学习到的决策边界，而无须构建具体的线性分离的高维空间。

### 示例代码

下面的代码演示了使用核方法的支持向量机如何学习呈圆形分布的数据的决策边界，我们生成圆形分布的数据，将其拆分为训练数据和验证数据。代码中没有明确指定使用哪个核方法，这是因为代码默认使用RBF（Radial Basis Function，径向基函数）核方法。

```python
from sklearn.svm import SVC
from sklearn.datasets import make_gaussian_quantiles
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# 数据生成
X, y = make_gaussian_quantiles(n_features=2, n_classes=2, n_samples=300)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy_score(y_pred, y_test)
```

### 代码注释

* from sklearn.datasets import make_gaussian_quantiles  # 导入生成高斯量化(正态分布)数据集的函数

* X, y = make_gaussian_quantiles(n_features=2, n_classes=2, n_samples=300) # 创建一个高斯量化数据集，包含2个特征，2个类别，总共300个样本


### 详细说明

学习结果因核函数而异

核方法中可以使用的核函数多种多样，使用不同的核函数，得到的决策边界的形状也不同，所示为使用具有代表性的核函数在相同数据上训练支持向量机的结果，这些核函数分别是线性核函数、Sigmoid核函数、多项式核函数和RBF核函数。

#### 注意点

使用核方法后，我们就无法清楚地知道支持向量机使用的是什么特征了。

核方法适合用于“相比特征的可解释性更看重精度”的场景，还需要注意的是，得到的决策边界不一定是通过实例代码学习到的那种一目了然的决策边界，由于这些特点，在使用支持向量机时，不宜立即使用非线性核函数，在此之前，应该首先使用线性核函数进行分析，以了解数据。
# 逻辑回归

逻辑回归是一种用于有监督学习的分类任务的简单算法，虽然算法的名字中含有“回归”二字，但其实它是用于分类问题的算法，逻辑回归通过计算数据属于各类别的概率来进行分类。

## 概述

逻辑回归是一种学习某个事件发生概率的算法，利用这个概率，可以对某个事件发生或者不发生进行二元分类。 虽然逻辑回归是二元分类的算法，但也可以用于三种类别以上的分类问题。


## 算法说明

逻辑回归根据数据$x$和表示其所属类别的标签$y$进行学习，计算概率。数据$x$可以当做由特征值组成的向量处理，如果标签是二元分类，则可以使用前面的$y=0, 1$，这种二元数值表示。

逻辑回归的基本思想与线性回归一致，对数据$x$乘以权重$w$，再加上偏置$w_0$，计算$w^Tx+w_0$的值，逻辑回归和线性回归在从数据中学习权重$w$和偏置$w_0$这一点上是相同的。

与线性回归不一样的地方在于，为了计算概率，逻辑回归的输出范围必须限制在0和1之间，逻辑回归使用Sigmoid函数$\sigma(z)=1/[1+exp(-z)]$，返回0和1之间的数值。

我们对输入数据$x$使用Sigmoid函数$\sigma(z)$，即使用$p=\sigma(w^Tx+w_0)$计算标签为$y$的概率$p$。二元分类通常将预测概率0.5作为阈值进行分类，当概率小于0.5时，将$y$的预测值分类为0，当概率大于0.5时，将$y$分类为1。当然根据问题的不同，有时会将阈值设置Wie大于或者小于0.5的值。

在学习过程中，我们使用逻辑损失作为误差函数进行最小化，与其他误差函数一样，逻辑损失是在分类失败时返回最大值，在分类成功时返回小值的函数，在与线性回归中引入的均方误差不同的是，我们无法通过式子变形来计算逻辑损失的最小值，因此需要采用梯度下降法通过数值计算来求解。

对于无法通过式子变形严密求解的情况，机器学习中经常会通过数值计算来近似求解。

### 示例代码

```python
import numpy as np
from sklearn.linear_model import LogisticRegression


X_train = np.r_[np.random.normal(3, 1, size=50), np.random.normal(-1, 1, size=50)].reshape((100, -1))
y_train = np.r_[np.ones(50), np.zeros(50)]
model = LogisticRegression()
model.fit(X_train, y_train)
model.predict_proba([[0], [1], [2]])[:, 1]
```

#### 部分代码解释

* X_train = np.r_[np.random.normal(3, 1, size=50), np.random.normal(-1, 1, size=50)].reshape((100, -1)) 
* 创建了一个包含100个样本的训练数据集。其中，np.random.normal用于生成正态分布的随机数。np.random.normal(3, 1, size=50)生成了50个均值为3，标准差为1的正态分布样本；np.random.normal(-1, 1, size=50)生成了50个均值为-1，标准差为1的正态分布样本。然后，这两组数据被合并（使用np.r_），并被重塑为一个100行1列的数组（reshape((100, -1))）。

* y_train = np.r_[np.ones(50), np.zeros(50)]
* 创建了一个长度为100的目标变量数组。前50个元素为1（表示一类），后50个元素为0（表示另一类）。这与上面创建的训练数据是对应的。


* model.predict_proba([[0], [1], [2]])[:, 1]
* 使用训练好的模型对三个新样本（值分别为0、1和2）进行预测，并返回这些样本属于类别1（标签为1）的概率。predict_proba方法返回每个样本属于各个类别的概率，这里通过[:, 1]选择了属于第二类（即类别1）的概率。


### 详细说明

在解决分类问题时，如果让学习后的模型对未知数据分类，模型就会以某个地方为边界来区分分类结果，这个结果称之为**决策边界**，逻辑回归的决策边界是计算出的概率正好为50%的地方。

决策边界的形状因使用的算法不同而有很大的不同，在平面的情况下，逻辑回归的决策边界是直线，在其他算法中，比如KNN和神经网络中，决策边界是更复杂的形式。


### 对特征的解释

可以查看逻辑回归中的每个特征的系数，通过查看每个特征的系数的符号，可以知道它对概率是正影响还是负影响。

# 线性回归

线性回归(linear regression)是用于预测回归问题的算法。算法中根据训练数据计算使损失最小的参数的做法是有监督学习算法的共同之处。

## 概述

线性回归是对“目标变量随着某个特征变量的增大而增大(或者减少)”这种关联性建模而得到的直线。

直线可写为$y=w_0+w_1x$，其中$w_1$为斜率，或者称之为权重，$w_0$相当于在y轴行的截距。斜率和截距是由有监督学习的算法学到的参数，所以我们称之为学习参数。

线性回归算法一般使用一个以上的特征变量创建模型，其中只有一个独立的特征变量的情况叫做一元回归。

## 算法说明

对于直线$y=w_0+w_1x$，只要给定不同的点，我们就能求出唯一的$w_0$和$w_1$，但是在线性回归中，我们需要根据不在一条直线上的点求出学习参数。

比如给出$y=0.706x+0.823$，$y=-0.125x+4.5$，如何评价这两条直线中的哪一条更好地表示了数据的关联性呢？

我们可以通过均方误差进行定量判断，均方误差指的是目标变量和直线的差$y_i-(w_0+w_1x)$的平方的平均值，当存在n个数据时，可如下表示：

$$
\begin{aligned}
\frac{\sum_{i=1}^{n}[y_i-(w_0+w_1x)]^2}{n}
\end{aligned}\tag{1}
$$

改变学习参数$w_0$和$w_1$，那么计算出的均方误差也会发生变化，这种表示误差和学习参数之间关系的函数叫做误差函数(或者损失函数)。线性回归需要再各条直线中找出使误差函数值最小的参数。

## 算法说明

```python
from sklearn.linear_model import LinearRegression


X = [[10.0], [8.0], [13.0], [9.0], [11.0], [14.0], [6.0], [4.0], [12.0], [7.0], [5.0]]
y = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]
model = LinearRegression()
model.fit(X, y)

# 截距
print(model.intercept_)

# 斜率
print(model.coef_)

y_pred = model.predict([[0], [1]])

# 对x=0, x=1的预测结果
print(y_pred)
```

代码说明：
1. 
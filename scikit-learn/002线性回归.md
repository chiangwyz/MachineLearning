# 线性回归

线性回归(linear regression)是用于预测回归问题的算法。算法中根据训练数据计算使损失最小的参数的做法是有监督学习算法的共同之处。

## 概述

线性回归是对“目标变量随着某个特征变量的增大而增大(或者减少)”这种关联性建模而得到的直线。

直线可写为$y=w_0+w_1x$，其中$w_1$为斜率，或者称之为权重，$w_0$相当于在y轴行的截距。斜率和截距是由有监督学习的算法学到的参数，所以我们称之为学习参数。

线性回归算法一般使用一个以上的特征变量创建模型，其中只有一个独立的特征变量的情况叫做一元回归。

## 算法说明

对于直线$y=w_0+w_1x$，只要给定不同的点，我们就能求出唯一的$w_0$和$w_1$，但是在线性回归中，我们需要根据不在一条直线上的点求出学习参数。

比如给出$y=0.706x+0.823$，$y=-0.125x+4.5$，如何评价这两条直线中的哪一条更好地表示了数据的关联性呢？

我们可以通过均方误差进行定量判断，均方误差指的是目标变量和直线的差$y_i-(w_0+w_1x)$的平方的平均值，当存在n个数据时，可如下表示：

$$
\begin{aligned}
\frac{\sum_{i=1}^{n}[y_i-(w_0+w_1x)]^2}{n}
\end{aligned}\tag{1}
$$

改变学习参数$w_0$和$w_1$，那么计算出的均方误差也会发生变化，这种表示误差和学习参数之间关系的函数叫做误差函数(或者损失函数)。线性回归需要再各条直线中找出使误差函数值最小的参数。

## 算法说明

```python
from sklearn.linear_model import LinearRegression

"""
X 代表的是线性回归模型的输入数据。在机器学习和统计建模中，输入数据通常表示为一个二维数组或矩阵，其中每一行代表一个数据点，
每一列代表一个特征。
"""
X = [[10.0], [8.0], [13.0], [9.0], [11.0], [14.0], [6.0], [4.0], [12.0], [7.0], [5.0]]
y = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]
model = LinearRegression()
model.fit(X, y)

# 截距
print(model.intercept_)

# 斜率
print(model.coef_)

y_pred = model.predict([[0], [1]])

# 对x=0, x=1的预测结果
print(y_pred)
```

## 详细说明

### 线性回归不成功的例子

某些数据看上去是曲线分布，所以假定这些数据时线性分布的就不合适；如果数据中有离群值，应该对其进行离群值的预处理，或者应用便于处理离群值的其他方法。**对原本不遵循线性分布的数据强行进行线性回归也得不到好的结果，所以首先应该进行可视化，再考虑是否进行线性回归。**


### 均方误差的最小化方法

均方误差可以使用学习参数的函数表示：

$$
\begin{aligned}
L(w_0, w_1)=
\frac{\sum_{i=1}^{n}[y_i-(w_0+w_1x)]^2}{n}
\end{aligned}\tag{2}
$$

举例为：

$$
L(w_0, w_1)=w^2_0+24.5w^2_1+9w_0w_1-8w_0-42w_1+21
$$

该式是$w_0$、$w_1$的二次函数。


### 各种线性回归和非线性回归

一元回归是指独立特征变量只有一个的线性回归，独立特征变量为两个及以上时的线性回归叫做多元回归。尽管独立特征变量只有一个，但包含$x^2, x^3$这种特征变量的次方项的线性回归叫做多项式回归。多项式回归对于特征变量$x_1$不是线性的，所以称之为“线性”回归可能让人觉得不太合适，但是是否作为线性回归不是从特征变量来看的，从学习参数的角度来看是线性的回归，所以我们才称之为线性回归，所以多项式回归也属于线性回归。

| 线性回归的种类 | 示例 |
|--------|-------------------|
|一元回归|$y=w_0+w_1x_1$|
|多元回归|$y=w_0+w_1x_1+w_2x_2$|
|多项式元回归|$y=w_0+w_1x_1+w_2x^2_1$|


而$y=e^{w_1x_1}$和$y=\frac{1}{w_1x_1+1}$为非线性回归。

# 概述

正则化是为了防止过拟合的一种方法，与线性回归等算法配合使用，通过向损失函数增加惩罚项的方式度力模型施加制约，有望提高模型的泛化能力。
**主要用于机器学习模型的训练阶段**，过拟合是指模型在验证数据上产生的误差比在训练数据上产生的误差（训练误差）大得多的现象。

不同次数的训练误差和验证误差如下表所示，可以看出随着函数次数的增加，训练误差逐渐变小了，但同事验证误差增加了。六次线性回归是一个复杂的模型，虽然它减少了训练误差，但是犹豫过拟合，所以它的泛华能力很低。

### 表 次数与训练误差、验证误差的关系
| 次数 | 训练误差 |验证误差 |
|--------|--------|--------|
|1|0.412|0.618|
|2|0.176|0.193|
|3|0.081|0.492|
|$\vdots$|$\vdots$|$\vdots$|
|6|0.024|3.472|

### 表 应用正则化后的次数与训练误差、验证误差的关系
| 次数 | 训练误差 |验证误差 |
|--------|--------|--------|
|1|0.412|0.618|
|2|0.372|0.532|
|3|0.301|0.394|
|$\vdots$|$\vdots$|$\vdots$|
|6|0.159|0.331|

目前存在很多正则化的方法，本书前面使用的回归模型是被称为岭回归(ridge regression)的具有代表性的回归方法。

## 算法说明

复杂模型的过拟合的一个原因是学习参数$w_i$的值太大或者太小。

为什么正则化可以抑制学习参数变大呢？这里以岭回归的误差函数为例进行说明。为了方便说明问题，以二次线性回归应用正则化的情况进行说明：

$$
\begin{aligned}
R(w)=
\sum_{i=1}^{n}[y_i-(w_0+w_1x_i+w_2x_i^2)]^2 + \alpha(w_1^2 + w_2^2)
\end{aligned}
$$

等号右边的第一项$\sum_{i=1}^{n}[y_i-(w_0+w_1x_i+w_2x_i^2)]^2$是线性回归的损失函数，第二项$\alpha(w_1^2 + w_2^2)$被称为惩罚项(或者正则化项)，是学习参数的平方和的形式，一般来说，惩罚项中不包含截距。

另外，$\alpha(\alpha \geq 0)$是控制正则化强度的参数，$\alpha$越大，对学习参数的抑制越强；$\alpha$越小，对训练数据过拟合的可能性越大。

下面思考岭回归的损失奇函数$R(w)$的最小化。

等号右边的第一项$\sum_{i=1}^{n}[y_i-(w_0+w_1x_i+w_2x_i^2)]^2$其实是求使得与训练数据$y$之间的误差变小的任意$w_0, w_1, w_2$的问题，右边第二项(即惩罚项)是学习参数的平方和，因此学习参数的绝对值越大，损失奇函数整体的值就越大。由此可知，惩罚项具有“对绝对值大学习参数给与损失变大的惩罚”的作用，这个作用可以抑制学习参数变大。

### 示例代码

下面是对sin函数进行岭回归建模的实力代码，代码中使用PolynomialFeatures方法创建了六次多项式。

```python
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error


train_size = 20
test_size = 12
train_X = np.random.uniform(low=0, high=1.2, size=train_size)
test_X = np.random.uniform(low=0.1, high=1.3, size=test_size)
train_y = np.sin(train_X * 2 * np.pi) + np.random.normal(0, 0.2, train_size)
test_y = np.sin(test_X * 2 * np.pi) + np.random.normal(0, 0.2, test_size)
poly = PolynomialFeatures(6) # 次数为6
train_poly_X = poly.fit_transform(train_X.reshape(train_size, 1))
test_poly_X = poly.fit_transform(test_X.reshape(test_size, 1))
model = Ridge(alpha=1.0)
model.fit(train_poly_X, train_y)
train_pred_y = model.predict(train_poly_X)
test_pred_y = model.predict(test_poly_X)
print(mean_squared_error(train_pred_y, train_y))
print(mean_squared_error(test_pred_y, test_y))
```

#### 代码解释

* PolynomialFeatures 用于生成给定数据集的多项式特征。这在进行多项式回归时非常有用，可以将数据转换为高维空间，从而拟合更复杂的模型。

* Ridge 是用于进行岭回归的类。岭回归是一种改进的线性回归，它通过引入正则化项来防止过拟合。

* mean_squared_error 用于计算预测值和实际值之间的均方误差，这是评估回归模型性能的常用指标。

* train_size = 20  # 设置训练集的大小为20
* test_size = 12   # 设置测试集的大小为12

* train_X = np.random.uniform(low=0, high=1.2, size=train_size) # 使用numpy生成train_size个在[0, 1.2)区间内均匀分布的随机数，作为训练集的特征

* test_X = np.random.uniform(low=0.1, high=1.3, size=test_size) # 使用numpy生成test_size个在[0.1, 1.3)区间内均匀分布的随机数，作为测试集的特征

* train_y = np.sin(train_X * 2 * np.pi) + np.random.normal(0, 0.2, train_size) # 对于每个训练集特征值train_X，计算其正弦值（乘以2π以调整周期），然后添加均值为0，标准差为0.2的正态分布噪声，生成训练集的目标值

* test_y = np.sin(test_X * 2 * np.pi) + np.random.normal(0, 0.2, test_size) # 对于每个测试集特征值test_X，计算其正弦值（乘以2π以调整周期），然后添加均值为0，标准差为0.2的正态分布噪声，生成测试集的目标值

* poly = PolynomialFeatures(6)  # 创建一个多项式特征生成器，指定多项式的最高次数为6

* train_poly_X = poly.fit_transform(train_X.reshape(train_size, 1)) # 将训练集数据 train_X 转换为其多项式特征。首先，使用 reshape 方法将 train_X 转换为列向量。然后，调用 fit_transform 方法在训练数据上学习多项式转换（计算必要的参数）并应用它，生成多项式特征。这些特征包括 train_X 的原始值、train_X 的平方、立方，一直到 6 次幂。

* test_poly_X = poly.fit_transform(test_X.reshape(test_size, 1)) # 对测试集数据 test_X 也执行同样的转换。首先将 test_X 转换为列向量，然后应用相同的多项式特征转换。注意这里也使用了 fit_transform，这实际上是不标准的做法，通常应该使用 transform，因为我们应该使用在训练集上学到的参数来转换测试集，而不是重新学习测试集上的参数。



#### 拓展知识


# 随机森林

随机森林(Random Forest)是将多个模型综合起来创建更高性能模型的方法，即可用于回归，也可以用于分类，同样的算法有梯度提升(gradient boosting)等在机器学习竞赛中很受欢迎的算法。

通过学习随机森林，我们可以学到在其它算法中也适用的基础知识。


## 概述

随机森林的目标是利用多个决策树模型，获得比单个决策树更高的预测精度。单个决策树的性能并不一定很高，但是多个决策树汇总起来，一定能创建泛华能力更强的模型。

随机森林即可用于回归，也可以用于分类。这里以分类问题为例进行说明，该算法从每个决策树收集输出，通过多数表决得到最终的结果。


随机森林的多数表决就像找别人商量事情一样，不止听一个人的意见，而是在听取许多人的意见之后综合判断。机器学习也一样，通过创建多个模型，采取多数表决的方式，可以期待获取更为妥当的结果。

需要注意的是，如果使用同样的学习方法创建决策树，那么输出的就都是同样的东西，也就失去了采取多数表决的意义。

## 算法说明

随机森林是综合决策树的结果的算法啊，本节先介绍决策树的基本内容，在介绍如何综合决策树的结果。

### 决策树

决策树是通过将训练数据按条件分支进行划分来解决分类问题的方法，在分割时利用了表示数据杂乱程度(或者不均衡程度)的不纯度的数值。决策树为了使的表示数据杂乱程度的不纯度变小，对数据进行分割。当分割出来的数据组中存在很多相同的标签时，不纯度会变小；反之，当分割出来的数据组中存在很多不多不同的标签时，不纯度会变大。

表示不纯度的具体指标有很多种，本章节使用基尼系数，基尼系数的计算方式如下：

$$
\begin{aligned}
基尼系数=1-\sum_{i=1}^{c}(p_i)^2
\end{aligned}
$$

$c$是标签数，$p_i$是某个标签的数据量占数据总数量的比例。

使用分割出的每个部分的基尼系数乘以该部分所含数据的数量所占的比例，得到加权平均值。

决策树的学习是通过反复分割空间来进行的，具体来说，就是重复如下步骤：

1. 计算某个区域的所有特征值和候选分割的不纯度；
2. 以分割时不纯度减少最多的分割方式分割区域；
3. 对于分割后的区域，重复步骤1和步骤2.


### 随机森林

假设有3棵独立的决策树，每棵的正确率为0.6，此时，对每棵决策树的结果进行多数表决，并将表决结果作为预测结果，即可提高正确率。在这种情况下，预测不正确有两种请：一种是所有的决策树都预测不正确(概率为$(1-0.6)^3=0.064$)；另一种是3棵决策树找那个有2棵预测不正确(概率为$3\cdot(1-0.6)^2\cdot0.6=0.288$)，所以正确的概率为1-0.064-0.288=0.648，可以看出，正确率变高了。

接下来，让我们思考如何使用相同的数据训练多棵独立的决策树，这其实并不简单，机器学习从相同的数据上学习的结果基本上一样的，即使有100棵决策树，如果它们的学习方法都一样，那么多数表决的结果也还是一样的。

随机森林尝试对每棵决策树的数据应用下述方法来进行训练，以使得分类结果尽可能地不同。先采用Bootstrap方法，根据训练数据生成多个不同内容的训练数据，所谓Bootstrap方法，即通过对单个训练数据进行多次随机的抽样放回，“虚增”训练数据，这样就可以为每棵决策树输入不同的训练数据，然后再根据使用Bootstrap方法创建的训练数据训练决策树时，只随机选取部分特征值来训练决策树，通过“Bootstrap方法”和“随机选取特征值”这两种方法，就可以训练处具有多样性的决策树。

随机森林利用这种方式创建多棵数据集、训练多棵决策树、对决策结果进行多数考核，返回最终的分类结果。

### 示例代码

```python
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# 数据生成
data = load_wine()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3)
model = RandomForestClassifier() 
model.fit(X_train, y_train) # 训练
y_pred = model.predict(X_test) 
accuracy_score(y_pred, y_test) # 评估
```


## 详细说明

### 特征的重要度








# 支持向量机

支持向量机(Support vector machine, SVM)，既可以用于分类，也可以用于回归。

本节介绍如何将线性支持向量机应用于二元分类问题，以间隔(margin)最大化为基准，得到更好的决策边界。


## 概述

本节使用线性支持向量机(Linear support vector machine, LSVM)处理二元分类，线性支持向量机是以间隔最大化为基准，来学习得到尽可能地远离数据的决策边界的算法。虽然该算法的决策边界与逻辑回归一样是线性的，但有时线性支持向量机得到的结果更好。

**线性支持向量机的学习方式是：以间隔最大化为基准，让决策边界尽可能地远离数据**

## 算法说明

线性支持向量机通过最大化间隔来获得更好的用于分类的决策边界，首先需要阐述下间隔的定义，为了方便起见，我们以平面上的二元分类问题为例进行说明，并且假设数据可以完全分类。LSVM通过线性的决策边界平面一分为二，此时，训练数据中最接近决策边界的数据与决策边界之间的距离就成为间隔。

## 示例代码

```python
from sklearn.svm import LinearSVC
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# 数据生成
centers = [(-1, -0.125), (0.5, 0.5)]
X, y = make_blobs(n_samples=50, n_features=2, centers=centers, cluster_std=0.3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
model = LinearSVC() 
model.fit(X_train, y_train) # 训练
y_pred = model.predict(X_test) 
accuracy_score(y_pred, y_test) # 评估

# 计算决策边界直线
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
w = model.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(x_min, x_max)
yy = a * xx - (model.intercept_[0]) / w[1]

# 可视化
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)
plt.title("Train Data and Test Data Visualization")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### 代码说明

* from sklearn.svm import LinearSVC: 导入用于分类的线性支持向量机。
* from sklearn.datasets import make_blobs: 用于生成随机数据的函数。
* from sklearn.model_selection import train_test_split: 用于将数据集分割为训练集和测试集。
* from sklearn.metrics import accuracy_score: 用于评估模型准确性的函数。

* centers = [(-1, -0.125), (0.5, 0.5)]: 定义两个中心点用于生成数据。
* X, y = make_blobs(n_samples=50, n_features=2, centers=centers, cluster_std=0.3): 生成一个包含50个样本的数据集，每个样本有两个特征。centers参数指定了数据的中心点，cluster_std是每个群集的标准差。

* X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3): 将数据集分割成训练集和测试集。测试集占总数据集的30%（test_size=0.3）。

* model = LinearSVC(): 创建一个线性SVM分类器的实例。
* model.fit(X_train, y_train): 使用训练数据（X_train, y_train）来训练

* y_pred = model.predict(X_test): 使用测试数据集X_test进行预测。
* accuracy_score(y_pred, y_test): 计算模型的准确度，即预测正确的比例。


## 软间隔与支持向量

在前述代码中，我们了解的都是数据可以线性分离的情况，这种不允许数据进入间隔内侧的情况成为硬间隔，但是一般来说，数据并不是完全可以线性分离的，所以要允许一部分数据进入间隔内侧，这种情况叫做软间隔。

基于线性支持向量机的学习结果，我们可以将训练数据分为以下3种。

1. 与决策边界之间的距离比间隔还要远的数据：间隔外侧的数据。
2. 与决策边界之间的距离和间隔相同的数据：间隔上的数据。
3. 与决策边界之间的距离比间隔近，或者误分类的数据：间隔内侧的数据。

### 示例代码

```python
from sklearn.svm import LinearSVC
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# 生成重叠数据
centers = [(-1, -0.125), (0.5, 0.5)]
X, y = make_blobs(n_samples=100, n_features=2, centers=centers, cluster_std=0.5)  # 增加标准差以增加重叠
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# 创建并训练软间隔SVM模型
model = LinearSVC(C=0.1)  # 使用较小的C值以创建更软的间隔
model.fit(X_train, y_train)

# 计算决策边界直线
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
w = model.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(x_min, x_max)
yy = a * xx - (model.intercept_[0]) / w[1]

# 绘制决策边界直线和数据点
plt.figure(figsize=(8, 6))
plt.plot(xx, yy, 'k-', label="Decision Boundary")
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, edgecolor='k', cmap='viridis', label="Training Data")
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, edgecolor='k', cmap='viridis', alpha=0.6, label="Test Data")
plt.title("SVM Decision Boundary with Soft Margin")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()

```


## 拓展知识

### make_blobs

#### 简介

make_blobs 是一个用于生成随机数据的函数，它被包含在 Python 的 Scikit-learn 库中。make_blobs 用于生成具有高斯分布（正态分布）的数据点。这些数据点可以被分布在不同的群集（clusters）中，使其适合于测试聚类算法和分类算法。

#### 主要参数:

* n_samples: 生成的总样本数。
* n_features: 每个样本的特征数量。例如，如果你想在二维空间中可视化数据，你可以设置 n_features=2。
* centers: 指定群集中心的数量或坐标。如果是一个整数，它代表群集的数量；如果是数组形式，则代表每个群集的确切中心坐标。
* cluster_std: 每个群集的标准差。这决定了群集中点的分散程度。可以为每个群集指定不同的标准差。
* center_box: 生成群集中心时的边界框。
* shuffle: 是否打乱样本的顺序。
* random_state: 控制随机数生成器的种子。它可以确保每次调用时生成相同的数据集。

#### 返回值

返回生成的样本和它们对应的群集标签。样本作为二维数组返回，每行代表一个样本，列数等于n_features。标签数组表示每个样本属于哪个群集。

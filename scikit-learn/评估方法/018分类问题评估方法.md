# 评估方法

## 有监督学习的评估

有监督学习有多种评估模型的指标，我们需要梳理一下现有的有监督学习的常见的评估方法，提高机器学习性能的方法，以及提高性能时的障碍。

评估有监督学习的分类问题和回归问题时，所需的指标是不同的。

| 分类的问题 | 回归的问题 |
|--------|--------|
|混淆矩阵|均方误差|
|正确率|决定系数|
|精确率||
|召回率||
|F值||
|AUC||


## 分类问题的评估方法

以美国威斯康星州乳腺癌数据集进行机器学习的代码为示例

```python
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
X = data.data
y = 1 - data.target
# 反转标签的0和1

X = X[:, :10]
from sklearn.linear_model import LogisticRegression
model_lor = LogisticRegression()
model_lor.fit(X, y)
y_pred = model_lor.predict(X)
```

## 混淆矩阵

在分类问题评估上使用的主要指标为混淆矩阵(confusion matrix)，混淆矩阵可以将分类结果以表格的形式汇总，这样就可以检查哪些标签分类正确，哪些标签分类错误。


```python
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print(cm)

# [[348   9]
#  [ 15 197]]
```

将这个二元分类的结果作为混淆矩阵输出，我们会得到一个2行2列的矩阵，它是一个真实数据(正确答案)和预测数据的矩阵，如下表所示：

|  |  |预测 | |
|--------|--------|--------|--------|
|||0|1|
|真实数据|0|TN|FP|
|真实数据|1|FN|TP|



其中，

TN(True Negative)指的是阴性数据实际被正确预测为阴性的情况(正确判断为良性)。

FP(False Positive)指的是阴性数据实际被错误预测为阳性的情况(良性被判断为恶性)。

FN(False Negative)指的是阳性数据实际被错误预测为阴性的情况(恶性被判断为良性)。

TP(True Positive)指的是阳性数据实际被正确预测为阳性的情况(正确判断为恶性)。

从输出的数据来看，TN有348，TP有197条，FP有9条，FN有15条。正确预测的TN、TP的值较大，总体良好。但是有15条FN，9条FP，说明有15个恶性患者被漏诊，再看一下相反的情况，FP有9条，说明有9个良性患者被预测为恶性了。


这一点可以通过召回率(recall)再次得到证实，如果想避免恶性患者被漏诊，可以通过预测概率来调整预测。

混淆矩阵由4个数值组成，作为评估指标使用可能让人比较难以理解，因此，有时我们会使用混淆矩阵中的元素计算其他数值，并将其作为评估指标，经常使用的指标有以下几个：

正确率：$\frac{TP+TN}{TP+TN+FP+FN}$

精确率：$\frac{TP}{TP+FP}$

召回率：$\frac{TP}{TP+FN}$

F值：2 * (精确率 * 召回率) / (精确率 + 召回率)

## 正确率

正确率(Accuracy)指的是预测正确的结果占总预测结果的比例，accuracy_score函数用于计算正确率。

```python
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y, y_pred)
print('Accuracy: {}'.format(accuracy))

# Accuracy: 0.9578207381370826
```

这里输出的是：基于“作为实际答案的目标变量y”和“使用学习后的模型预测的y_pred”计算出来的正确率。

正确率超过了90%，说明模型正确地学习了数据。

## 精确率

精确率(Precision)指的是在所有被预测为阳性的数据中，被正确预测为阳性的数据所占的比例，precision_score函数用于计算精确率。
```python
from sklearn.metrics import precision_score

precision = precision_score(y, y_pred)
print('precision: {}'.format(precision))

# precision: 0.9563106796116505
```

与正确率的计算一样，精确率是基于y和y_pred计算出来的，在这个问题中，它表示预测为恶性的病例中实际确实为恶性的病例所占的比例。

## 召回率

召回率指的是在实际为阳性的数据中，被正确预测为阳性的数据所占的比例，recall_score函数用于计算召回率。

```python
from sklearn.metrics import recall_score

recall = recall_score(y, y_pred)
print('recall: {}'.format(recall))

# recall: 0.9292452830188679
```

召回率是基于y和y_pred计算出来的，在这个问题中，它表示为实际为恶性的病例被正确地预测为恶性的病例所占的比例，召回率低，意味着实际为恶性的病例大部分被预测为良性。与精确率低的情况相比，召回率低的问题更严重。


## F值

F值是综合反映精确率和召回率两个确实的指标，f1_score函数用于计算F值。

```python
from sklearn.metrics import f1_score
f1 = f1_score(y, y_pred)
print('f1: {}'.format(f1))

# f1: 0.9425837320574162
```